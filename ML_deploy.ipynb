{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GulnazaS/Deploy_model/blob/main/ML_deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentencepiece accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install peft\n",
        "!pip install llama-index-readers-wikipedia wikipedia"
      ],
      "metadata": {
        "id": "w_R63qEQfSHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "auo0hE4FqNkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "from pyvis.network import Network\n",
        "from huggingface_hub import login\n",
        "HF_TOKEN=\"HFL\"\n",
        "# Вставьте ваш токен (здесь указан временный токен)\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "38mnk36gbPmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "buHzPVhGHD6Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "G5M1SbjosPmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  print(docs)"
      ],
      "metadata": {
        "id": "79wQ0Set70JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Загрузка страниц из википедии\n",
        "from llama_index.readers.wikipedia import WikipediaReader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import re\n",
        "\n",
        "# Загружаем модель для исправления орфографических ошибок\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sage-fredt5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai-forever/sage-fredt5-large\", device_map='auto')\n",
        "\n",
        "def process_documents(docs):\n",
        "    def clean_text(text):\n",
        "        # Удаление HTML-тегов, если они есть\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        # Удаление всех символов, кроме букв и пробелов\n",
        "        text = re.sub(r'[^а-яА-ЯёЁ\\s]', '', text)\n",
        "        # Удаление лишних пробелов\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    # Применение функции очистки к каждому документу\n",
        "    cleaned_docs = [clean_text(doc.text) for doc in docs]\n",
        "\n",
        "    # Возвращаем очищенный текст\n",
        "    return \"\\n\\n\".join(cleaned_docs)\n",
        "\n",
        "# Инициализация объекта WikipediaReader\n",
        "reader = WikipediaReader()\n",
        "\n",
        "def correct_text(input_text):\n",
        "    inputs = tokenizer(input_text, max_length=512, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs.to(model.device), max_length=inputs[\"input_ids\"].size(1) * 1.5)\n",
        "    corrected_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return corrected_text[0]  # Возвращаем первый элемент, так как batch_decode возвращает список\n",
        "\n",
        "def load_wikipedia_data():\n",
        "    # Загрузка данных из Википедии\n",
        "    docs = reader.load_data(\n",
        "        pages=[\"История искусства\"],  # запрос раздела на тему ИИ\n",
        "        lang_prefix='ru'\n",
        "    )\n",
        "    return docs\n",
        "\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# Исправление орфографических ошибок и парсинг текста\")\n",
        "    with gr.Row():\n",
        "        output_text = gr.Textbox(lines=5, placeholder=\"Данные из Википедии будут здесь\")\n",
        "        parse_text = gr.Textbox(lines=5, placeholder=\"Распарсенный текст будет здесь\")\n",
        "    with gr.Row():\n",
        "        load_button = gr.Button(\"Загрузить данные из Википедии\")\n",
        "        parse_button = gr.Button(\"Распарсить текст\")\n",
        "    with gr.Row():\n",
        "       corrected_textbox = gr.Textbox(lines=5, placeholder=\"Исправленный текст будет отображаться здесь\")\n",
        "    with gr.Row():\n",
        "        correct_button = gr.Button(\"Исправить текст\")\n",
        "\n",
        "    load_button.click(fn=load_wikipedia_data, outputs=output_text)\n",
        "    parse_button.click(fn=lambda: process_documents(load_wikipedia_data()), outputs=parse_text)\n",
        "    correct_button.click(fn=correct_text, inputs=parse_text, outputs=corrected_textbox)\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "id": "exOtg7GXyWZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}